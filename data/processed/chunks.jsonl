{"chunk_id": "chunk_1_1", "unit_id": "unit_1", "unit_name": "Introduction to Data Structures", "text": "Arrays are fundamental data structures that store elements in contiguous memory locations. They provide O(1) access time for random access using indices. Arrays support various operations including insertion, deletion, searching, and traversal. The time complexity for insertion and deletion at the end is O(1), but inserting or deleting at arbitrary positions requires O(n) time due to element shifting. Arrays are best suited for scenarios where the size is known in advance and frequent access by index is required.", "page": 1, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_1_2", "unit_id": "unit_1", "unit_name": "Introduction to Data Structures", "text": "Linked Lists are linear data structures where elements are stored in nodes, and each node points to the next node in the sequence. Unlike arrays, linked lists do not require contiguous memory allocation. There are three main types: Singly Linked Lists (each node points to the next), Doubly Linked Lists (nodes have pointers to both next and previous nodes), and Circular Linked Lists (the last node points back to the first). Insertion and deletion operations are more efficient in linked lists compared to arrays, with O(1) time complexity when the position is known.", "page": 2, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_1_3", "unit_id": "unit_1", "unit_name": "Introduction to Data Structures", "text": "Stack is a Last-In-First-Out (LIFO) data structure that supports two primary operations: push (insert element at top) and pop (remove element from top). Stacks can be implemented using arrays or linked lists. Common applications include function call management, expression evaluation, backtracking algorithms, and undo mechanisms in editors. The time complexity for both push and pop operations is O(1). Additional operations include peek (view top element without removal) and isEmpty to check if the stack is empty.", "page": 3, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_1_4", "unit_id": "unit_1", "unit_name": "Introduction to Data Structures", "text": "Queue is a First-In-First-Out (FIFO) data structure with two main operations: enqueue (insert at rear) and dequeue (remove from front). Queues can be implemented using arrays or linked lists. Circular queues optimize space utilization by reusing empty positions. Priority queues are a variant where elements are dequeued based on priority rather than insertion order. Queues are used in scheduling algorithms, breadth-first search, buffer management, and handling asynchronous data transfers. Both enqueue and dequeue operations have O(1) time complexity in efficient implementations.", "page": 4, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_1_5", "unit_id": "unit_1", "unit_name": "Introduction to Data Structures", "text": "Time Complexity Analysis measures the amount of time an algorithm takes as a function of input size. Common time complexities include O(1) for constant time, O(log n) for logarithmic time, O(n) for linear time, O(n log n) for linearithmic time, O(n²) for quadratic time, and O(2ⁿ) for exponential time. Space Complexity measures the amount of memory an algorithm uses. Big-O notation provides an upper bound on growth rate, while Omega provides a lower bound and Theta provides a tight bound. Analyzing complexity helps in choosing the most efficient algorithm for a given problem.", "page": 5, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_2_1", "unit_id": "unit_2", "unit_name": "Trees and Graphs", "text": "Binary Trees are hierarchical data structures where each node has at most two children, referred to as left child and right child. A Binary Search Tree (BST) is a special binary tree where the left subtree contains values less than the parent node, and the right subtree contains values greater than the parent node. This property enables efficient searching, insertion, and deletion with average time complexity of O(log n). However, in the worst case of a skewed tree, operations can degrade to O(n). BSTs are widely used in databases, file systems, and implementing sets and maps.", "page": 10, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_2_2", "unit_id": "unit_2", "unit_name": "Trees and Graphs", "text": "Tree Traversals are systematic ways to visit all nodes in a tree. Inorder traversal visits left subtree, then root, then right subtree, producing sorted output for BSTs. Preorder traversal visits root first, then left and right subtrees, useful for creating a copy of the tree. Postorder traversal visits left and right subtrees before the root, used in deleting trees and evaluating postfix expressions. Each traversal can be implemented recursively or iteratively using a stack. The time complexity for all traversals is O(n) where n is the number of nodes, as each node is visited exactly once.", "page": 11, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_2_3", "unit_id": "unit_2", "unit_name": "Trees and Graphs", "text": "AVL Trees are self-balancing Binary Search Trees where the height difference between left and right subtrees (balance factor) is at most 1 for every node. When insertions or deletions cause imbalance, the tree performs rotations (single or double) to restore balance. This ensures that operations maintain O(log n) time complexity even in the worst case. Red-Black Trees are another self-balancing BST variant that use color properties and rotation rules. While AVL trees are more rigidly balanced, Red-Black trees provide faster insertion and deletion at the cost of slightly slower search operations.", "page": 12, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_2_4", "unit_id": "unit_2", "unit_name": "Trees and Graphs", "text": "Graphs are non-linear data structures consisting of vertices (nodes) and edges (connections between nodes). Graphs can be directed (edges have direction) or undirected (bidirectional edges), weighted (edges have values) or unweighted. Graphs can be represented using Adjacency Matrix (2D array where matrix[i][j] indicates edge between vertices i and j) or Adjacency List (array of lists where each index stores neighbors of that vertex). Adjacency matrices use O(V²) space and provide O(1) edge lookup, while adjacency lists use O(V+E) space and are more efficient for sparse graphs.", "page": 13, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_2_5", "unit_id": "unit_2", "unit_name": "Trees and Graphs", "text": "Breadth-First Search (BFS) explores a graph level by level, starting from a source vertex and visiting all neighbors before moving to the next level. BFS uses a queue data structure and is implemented iteratively. It finds the shortest path in unweighted graphs and is used in networking, social network analysis, and web crawling. Depth-First Search (DFS) explores as far as possible along each branch before backtracking. DFS uses a stack (or recursion) and is useful for detecting cycles, topological sorting, and finding connected components. Both algorithms have O(V+E) time complexity where V is vertices and E is edges.", "page": 14, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_3_1", "unit_id": "unit_3", "unit_name": "Sorting and Searching", "text": "Bubble Sort is a simple comparison-based sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This process continues until the list is sorted. Selection Sort divides the array into sorted and unsorted portions, repeatedly selecting the minimum element from the unsorted portion and moving it to the sorted portion. Insertion Sort builds the sorted array one element at a time by inserting each element into its correct position. All three algorithms have O(n²) worst-case time complexity, making them inefficient for large datasets but useful for small arrays or educational purposes.", "page": 20, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_3_2", "unit_id": "unit_3", "unit_name": "Sorting and Searching", "text": "Merge Sort is a divide-and-conquer algorithm that divides the array into two halves, recursively sorts each half, and then merges the sorted halves. It guarantees O(n log n) time complexity in all cases but requires O(n) additional space for merging. Quick Sort selects a pivot element, partitions the array so that elements smaller than the pivot come before it and larger elements come after, then recursively sorts the partitions. Quick Sort has O(n log n) average-case complexity but O(n²) worst-case when the pivot selection is poor. Despite this, Quick Sort is often faster in practice due to good cache locality and in-place partitioning.", "page": 21, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_3_3", "unit_id": "unit_3", "unit_name": "Sorting and Searching", "text": "Heap Sort uses a binary heap data structure to sort elements. It first builds a max heap from the input array, then repeatedly extracts the maximum element and places it at the end of the array. Heap Sort has O(n log n) time complexity in all cases and sorts in-place with O(1) extra space. Radix Sort is a non-comparison based algorithm that sorts integers by processing individual digits. It processes digits from least significant to most significant (or vice versa) using a stable sorting algorithm like counting sort for each digit position. Radix Sort achieves O(d·n) time complexity where d is the number of digits, making it faster than comparison-based sorts for certain integer datasets.", "page": 22, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_3_4", "unit_id": "unit_3", "unit_name": "Sorting and Searching", "text": "Linear Search is the simplest searching algorithm that checks every element in the array sequentially until the target element is found or the end is reached. It works on both sorted and unsorted arrays with O(n) time complexity. Binary Search is a much more efficient algorithm that works only on sorted arrays. It repeatedly divides the search interval in half by comparing the middle element with the target value. If the target is less than the middle element, the search continues in the left half; otherwise, it continues in the right half. Binary Search achieves O(log n) time complexity, making it extremely efficient for large sorted datasets.", "page": 23, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_3_5", "unit_id": "unit_3", "unit_name": "Sorting and Searching", "text": "Hashing is a technique that maps data of arbitrary size to fixed-size values using a hash function. Hash tables provide average O(1) time complexity for insertion, deletion, and search operations. Collision resolution techniques handle cases where multiple keys hash to the same location. Chaining uses linked lists at each hash table index to store multiple elements, while open addressing finds alternative locations within the table using probing sequences (linear probing, quadratic probing, or double hashing). The load factor (number of elements / table size) affects performance - when it exceeds a threshold, the table should be resized and all elements rehashed. Hash tables are fundamental to implementing dictionaries, caches, and database indexing.", "page": 24, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_4_1", "unit_id": "unit_4", "unit_name": "Advanced Data Structures", "text": "Heaps are complete binary trees that satisfy the heap property: in a max heap, each parent node is greater than or equal to its children; in a min heap, each parent is smaller than or equal to its children. Heaps are efficiently implemented using arrays where for a node at index i, its left child is at 2i+1 and right child is at 2i+2. Priority Queues are abstract data types typically implemented using heaps that allow insertion of elements with priorities and extraction of the highest (or lowest) priority element. Heap operations like insertion and deletion have O(log n) complexity. Priority queues are crucial in algorithms like Dijkstra's shortest path, Huffman coding, and task scheduling systems.", "page": 30, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_4_2", "unit_id": "unit_4", "unit_name": "Advanced Data Structures", "text": "B-Trees are self-balancing tree data structures that maintain sorted data and allow searches, sequential access, insertions, and deletions in logarithmic time. Unlike binary trees, B-tree nodes can have multiple children (determined by the order of the tree). B-Trees are optimized for systems that read and write large blocks of data, making them ideal for databases and file systems. B+ Trees are a variant where all data is stored in leaf nodes, with internal nodes only storing keys for navigation. This structure enables efficient range queries and sequential access. Leaf nodes in B+ trees are typically linked, facilitating ordered traversal. Database indexes commonly use B+ trees for their superior performance in disk-based storage.", "page": 31, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_4_3", "unit_id": "unit_4", "unit_name": "Advanced Data Structures", "text": "Tries, also called prefix trees, are tree-like data structures used to store strings where each path from root to leaf represents a word or prefix. Each node contains links to possible next characters and optionally marks the end of a word. Tries enable efficient prefix-based searching, autocomplete functionality, and spell checking with O(m) time complexity where m is the string length. Suffix Trees are compressed tries of all suffixes of a string, useful in pattern matching and bioinformatics. They support finding all occurrences of a pattern in O(m) time after O(n) preprocessing. Suffix trees are powerful but memory-intensive; suffix arrays provide a space-efficient alternative with slightly slower query times.", "page": 32, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_4_4", "unit_id": "unit_4", "unit_name": "Advanced Data Structures", "text": "Disjoint Set Union (DSU), also known as Union-Find, is a data structure that efficiently maintains a collection of disjoint sets and supports two primary operations: find (determine which set an element belongs to) and union (merge two sets). Using path compression during find operations and union by rank/size during union operations, both operations achieve nearly constant amortized time complexity O(α(n)) where α is the inverse Ackermann function, which grows extremely slowly. DSU is essential for Kruskal's minimum spanning tree algorithm, detecting cycles in undirected graphs, and solving connectivity problems in network design and image processing applications.", "page": 33, "source": "sample_syllabus.pdf"}
{"chunk_id": "chunk_4_5", "unit_id": "unit_4", "unit_name": "Advanced Data Structures", "text": "Advanced Graph Algorithms include Dijkstra's algorithm for finding shortest paths from a source to all other vertices in weighted graphs (O((V+E) log V) with priority queue). Bellman-Ford algorithm handles negative edge weights and detects negative cycles with O(VE) complexity. Floyd-Warshall finds shortest paths between all pairs of vertices in O(V³) time. Prim's and Kruskal's algorithms find minimum spanning trees - Prim's is greedy starting from a vertex (O((V+E) log V)), while Kruskal's sorts edges and uses DSU (O(E log E)). Topological sorting orders vertices in a directed acyclic graph such that for every edge u→v, u comes before v. These algorithms are fundamental in network routing, circuit design, project scheduling, and optimization problems.", "page": 34, "source": "sample_syllabus.pdf"}
